{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook:\n",
    "Manually annotated dataset of 100 positive and 100 negative labels with hypothesis \"This text is racist\" from VNN forum are used to train Logistic Regression and Random Forrest models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('/mnt/c/Yose/Data/vnn_data/active_learning/')\n",
    "df = pd.read_csv(data_folder / 'df_labeled_racism.tsv',  sep = '\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in relevant columns from df into X (embeddings) and y (labels) -> train|val|test split - 60|20|20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['chunk_embedding'], df['racist_text']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: np.fromstring(x[1:-1], sep=' ')).tolist() # transform X from str of embeddings to np array\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_params = {\n",
    "    'n_neighbors':20,\n",
    "    'n_components':3,\n",
    "    'min_dist':0.05, \n",
    "    'metric':'cosine'\n",
    "}\n",
    "\n",
    "X_umap = UMAP(**umap_params).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['umap_x'] = X_umap[:, 0]\n",
    "df['umap_y'] = X_umap[:, 1]\n",
    "df['umap_z'] = X_umap[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    df,\n",
    "    x=\"umap_x\",\n",
    "    y=\"umap_y\",\n",
    "    z=\"umap_z\",\n",
    "    color=\"racist_text\",\n",
    "    title=\"200 annotated embeddings with hypotesis 'This text is racist'\",\n",
    "    hover_data={\"index\": df.index, \"umap_x\": False, \"umap_y\": False, \"umap_z\": False},\n",
    "    width=1000,\n",
    "    height=500,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Logistic regression model\n",
    "Embeddings are alrady normalized and do not need scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning of hyperparameters: 'l1-ratio', 'C'\n",
    "param_grid = {\"l1_ratio\": [0, 0.1, 0.5, 1], \"C\": [0.01, 0.05, 0.08, 0.1, 0.12, 0.15, 0.3]} \n",
    "model_lr = GridSearchCV(LogisticRegression(penalty = 'elasticnet', solver='saga', max_iter=10000), param_grid=param_grid, cv=5, scoring=\"f1\")\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_lr.best_params_\n",
    "\n",
    "# Output: {'C': 0.1, 'l1_ratio': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with chosen parameters: l1-ratio = 0 (<=> penalty = l2), C = 0.1\n",
    "# calculate scores for model on train and val data\n",
    "\n",
    "model_lr = LogisticRegression(penalty = 'l2', solver='saga', max_iter=10000, C=0.1)\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_val)\n",
    "\n",
    "print(f'Score train: {100 * model_lr.score(X_train, y_train):.2f} %')\n",
    "print(f'Score val: {100 * model_lr.score(X_val, y_val):.2f} %')\n",
    "\n",
    "# Output example: Score train: 97.50 %, Score val: 95.00 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on train_val data with paramters from model tuning and evalute using text data\n",
    "\n",
    "model_lr = LogisticRegression(penalty = 'l2', solver='saga', max_iter=10000, C=0.1)\n",
    "model_lr.fit(X_trainval, y_trainval)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "print(f'Score trainval: {100 * model_lr.score(X_trainval, y_trainval):.2f} %')\n",
    "print(f'Score test: {100 * model_lr.score(X_test, y_test):.2f} %')\n",
    "\n",
    "# Output example: Score trainval: 94.38 %, Score test: 95.00 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# confusion matix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "ConfusionMatrixDisplay(cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes where y_test and y_pred do not match and save into false_neg and false_pos\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "comparison = (y_test != y_pred_lr)\n",
    "mismatched_indexes = comparison[comparison].index.tolist()\n",
    "y_pred_lr_series = pd.Series(y_pred_lr, index=y_test.index) # array -> Series so can index same as y_test\n",
    "false_pos = [index for index in mismatched_indexes if y_test[index] == 0 and y_pred_lr_series[index] == 1]\n",
    "false_neg = [index for index in mismatched_indexes if y_test[index] == 1 and y_pred_lr_series[index] == 0]\n",
    "\n",
    "print(f\"False positives: \\n{df.loc[false_pos, ['article_id', 'chunk_index', 'text_chunk', 'racist_text']]}\")\n",
    "print(f\"False negatives: \\n{df.loc[false_neg, ['article_id', 'chunk_index', 'text_chunk', 'racist_text']]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LogReg model on all data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on all data and save\n",
    "model_lr = LogisticRegression(penalty = 'l2', solver='saga', max_iter=10000, C=0.1)\n",
    "model_lr.fit(X,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_lr, data_folder / 'model_supervised_lr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Random forrest model\n",
    "Random Forrest does not require scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [100, 120, 150, 200], \"max_depth\": [2, 3, 4, 5]} \n",
    "model_rf = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=5, verbose=1, scoring=\"f1\")\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_rf.best_params_\n",
    "\n",
    "# Output: {'max_depth': 5, 'n_estimators': 120}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reduce risk for overfitting chosen parameters are: lower max_depth and higher n_estimators compared to results from GridSearchCV\n",
    "# calculate scores for model on train and val data\n",
    "model_rf = RandomForestClassifier(n_estimators=150, max_depth=3)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y1_pred_rf = model_rf.predict(X_val)\n",
    "\n",
    "print(f'Score train: {100 * model_rf.score(X_train, y_train):.2f} %')\n",
    "print(f'Score val: {100 * model_rf.score(X_val, y_val):.2f} %')\n",
    "\n",
    "# Output example: Score train: 100.00 %, Score val: 95.00 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on train_val data with paramters from model tuning and evalute using text data\n",
    "model_rf = RandomForestClassifier(n_estimators=150, max_depth=3)\n",
    "model_rf.fit(X_trainval, y_trainval)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "print(f'Score trainval: {100 * model_rf.score(X_trainval, y_trainval):.2f} %')\n",
    "print(f'Score test: {100 * model_rf.score(X_test, y_test):.2f} %')\n",
    "\n",
    "# Output example: Score trainval: 100.00 %, Score test: 95.00 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Random Forrest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# confusion matix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "ConfusionMatrixDisplay(cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indexes where y_test and y_pred do not match and save into false_neg and false_pos\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "comparison = (y_test != y_pred_rf)\n",
    "mismatched_indexes = comparison[comparison].index.tolist()\n",
    "y_pred_rf_series = pd.Series(y_pred_rf, index=y_test.index) # array -> Series so can index same as y_test\n",
    "false_pos = [index for index in mismatched_indexes if y_test[index] == 0 and y_pred_rf_series[index] == 1]\n",
    "false_neg = [index for index in mismatched_indexes if y_test[index] == 1 and y_pred_rf_series[index] == 0]\n",
    "\n",
    "print(f\"False positives: \\n{df.loc[false_pos, ['article_id', 'chunk_index', 'text_chunk', 'racist_text']]}\")\n",
    "print(f\"False negatives: \\n{df.loc[false_neg, ['article_id', 'chunk_index', 'text_chunk', 'racist_text']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RandomForrest model on all data and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=150, max_depth=3)\n",
    "model_rf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model_rf, data_folder / 'model_supervised_rf.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
